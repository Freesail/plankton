{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import make_grid\n",
    "from preprocess import PadToSquare\n",
    "from model import get_model\n",
    "from tools import imshow\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/raw_data'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "30336 121\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    PadToSquare(), \n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "dataset = datasets.ImageFolder(DATA_DIR + '/train/', data_transforms)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "class_names = dataset.classes\n",
    "print(len(dataset), len(dataset.classes))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "# inputs, classes = next(iter(dataloader))\n",
    "# out = make_grid(inputs)\n",
    "# imshow(out, title=[class_names[x] for x in classes])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['acantharia_protist', 'acantharia_protist_big_center', 'acantharia_protist_halo', 'amphipods', 'appendicularian_fritillaridae', 'appendicularian_s_shape', 'appendicularian_slight_curve', 'appendicularian_straight', 'artifacts', 'artifacts_edge', 'chaetognath_non_sagitta', 'chaetognath_other', 'chaetognath_sagitta', 'chordate_type1', 'copepod_calanoid', 'copepod_calanoid_eggs', 'copepod_calanoid_eucalanus', 'copepod_calanoid_flatheads', 'copepod_calanoid_frillyAntennae', 'copepod_calanoid_large', 'copepod_calanoid_large_side_antennatucked', 'copepod_calanoid_octomoms', 'copepod_calanoid_small_longantennae', 'copepod_cyclopoid_copilia', 'copepod_cyclopoid_oithona', 'copepod_cyclopoid_oithona_eggs', 'copepod_other', 'crustacean_other', 'ctenophore_cestid', 'ctenophore_cydippid_no_tentacles', 'ctenophore_cydippid_tentacles', 'ctenophore_lobate', 'decapods', 'detritus_blob', 'detritus_filamentous', 'detritus_other', 'diatom_chain_string', 'diatom_chain_tube', 'echinoderm_larva_pluteus_brittlestar', 'echinoderm_larva_pluteus_early', 'echinoderm_larva_pluteus_typeC', 'echinoderm_larva_pluteus_urchin', 'echinoderm_larva_seastar_bipinnaria', 'echinoderm_larva_seastar_brachiolaria', 'echinoderm_seacucumber_auricularia_larva', 'echinopluteus', 'ephyra', 'euphausiids', 'euphausiids_young', 'fecal_pellet', 'fish_larvae_deep_body', 'fish_larvae_leptocephali', 'fish_larvae_medium_body', 'fish_larvae_myctophids', 'fish_larvae_thin_body', 'fish_larvae_very_thin_body', 'heteropod', 'hydromedusae_aglaura', 'hydromedusae_bell_and_tentacles', 'hydromedusae_h15', 'hydromedusae_haliscera', 'hydromedusae_haliscera_small_sideview', 'hydromedusae_liriope', 'hydromedusae_narco_dark', 'hydromedusae_narco_young', 'hydromedusae_narcomedusae', 'hydromedusae_other', 'hydromedusae_partial_dark', 'hydromedusae_shapeA', 'hydromedusae_shapeA_sideview_small', 'hydromedusae_shapeB', 'hydromedusae_sideview_big', 'hydromedusae_solmaris', 'hydromedusae_solmundella', 'hydromedusae_typeD', 'hydromedusae_typeD_bell_and_tentacles', 'hydromedusae_typeE', 'hydromedusae_typeF', 'invertebrate_larvae_other_A', 'invertebrate_larvae_other_B', 'jellies_tentacles', 'polychaete', 'protist_dark_center', 'protist_fuzzy_olive', 'protist_noctiluca', 'protist_other', 'protist_star', 'pteropod_butterfly', 'pteropod_theco_dev_seq', 'pteropod_triangle', 'radiolarian_chain', 'radiolarian_colony', 'shrimp-like_other', 'shrimp_caridean', 'shrimp_sergestidae', 'shrimp_zoea', 'siphonophore_calycophoran_abylidae', 'siphonophore_calycophoran_rocketship_adult', 'siphonophore_calycophoran_rocketship_young', 'siphonophore_calycophoran_sphaeronectes', 'siphonophore_calycophoran_sphaeronectes_stem', 'siphonophore_calycophoran_sphaeronectes_young', 'siphonophore_other_parts', 'siphonophore_partial', 'siphonophore_physonect', 'siphonophore_physonect_young', 'stomatopod', 'tornaria_acorn_worm_larvae', 'trichodesmium_bowtie', 'trichodesmium_multiple', 'trichodesmium_puff', 'trichodesmium_tuft', 'trochophore_larvae', 'tunicate_doliolid', 'tunicate_doliolid_nurse', 'tunicate_partial', 'tunicate_salp', 'tunicate_salp_chains', 'unknown_blobs_and_smudges', 'unknown_sticks', 'unknown_unclassified']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(class_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "batch_per_disp = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [],
   "source": [
    "model = get_model('resnet18', [128, 128], len(class_names))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Epoch 0/0\n",
      "----------\n",
      "batch 0: loss 4.786 | acc 0.008\n",
      "batch 1: loss 4.772 | acc 0.008\n",
      "batch 2: loss 4.778 | acc 0.008\n",
      "batch 3: loss 4.766 | acc 0.031\n",
      "batch 4: loss 4.725 | acc 0.055\n",
      "batch 5: loss 4.732 | acc 0.062\n",
      "batch 6: loss 4.708 | acc 0.086\n",
      "batch 7: loss 4.670 | acc 0.062\n",
      "batch 8: loss 4.647 | acc 0.078\n",
      "batch 9: loss 4.663 | acc 0.062\n",
      "batch 10: loss 4.596 | acc 0.109\n",
      "batch 11: loss 4.597 | acc 0.070\n",
      "batch 12: loss 4.606 | acc 0.109\n",
      "batch 13: loss 4.561 | acc 0.094\n",
      "batch 14: loss 4.550 | acc 0.078\n",
      "batch 15: loss 4.515 | acc 0.078\n",
      "batch 16: loss 4.419 | acc 0.109\n",
      "batch 17: loss 4.388 | acc 0.125\n",
      "batch 18: loss 4.404 | acc 0.125\n",
      "batch 19: loss 4.351 | acc 0.125\n",
      "batch 20: loss 4.299 | acc 0.125\n",
      "batch 21: loss 4.388 | acc 0.125\n",
      "batch 22: loss 4.392 | acc 0.109\n",
      "batch 23: loss 4.352 | acc 0.094\n",
      "batch 24: loss 4.275 | acc 0.133\n",
      "batch 25: loss 4.250 | acc 0.156\n",
      "batch 26: loss 4.226 | acc 0.133\n",
      "batch 27: loss 4.282 | acc 0.125\n",
      "batch 28: loss 4.386 | acc 0.094\n",
      "batch 29: loss 4.325 | acc 0.062\n",
      "batch 30: loss 4.198 | acc 0.133\n",
      "batch 31: loss 4.190 | acc 0.148\n",
      "batch 32: loss 4.190 | acc 0.125\n",
      "batch 33: loss 4.033 | acc 0.117\n",
      "batch 34: loss 4.228 | acc 0.078\n",
      "batch 35: loss 4.190 | acc 0.188\n",
      "batch 36: loss 4.080 | acc 0.156\n",
      "batch 37: loss 3.973 | acc 0.164\n",
      "batch 38: loss 4.229 | acc 0.102\n",
      "batch 39: loss 4.156 | acc 0.117\n",
      "batch 40: loss 4.107 | acc 0.141\n",
      "batch 41: loss 4.306 | acc 0.070\n",
      "batch 42: loss 4.031 | acc 0.156\n",
      "batch 43: loss 3.914 | acc 0.203\n",
      "batch 44: loss 4.049 | acc 0.188\n",
      "batch 45: loss 3.985 | acc 0.180\n",
      "batch 46: loss 4.033 | acc 0.188\n",
      "batch 47: loss 3.979 | acc 0.203\n",
      "batch 48: loss 4.007 | acc 0.156\n",
      "batch 49: loss 3.725 | acc 0.234\n",
      "batch 50: loss 4.067 | acc 0.133\n",
      "batch 51: loss 3.918 | acc 0.141\n",
      "batch 52: loss 3.804 | acc 0.195\n",
      "batch 53: loss 3.528 | acc 0.250\n",
      "batch 54: loss 3.937 | acc 0.117\n",
      "batch 55: loss 3.637 | acc 0.211\n",
      "batch 56: loss 3.875 | acc 0.164\n",
      "batch 57: loss 3.908 | acc 0.148\n",
      "batch 58: loss 3.722 | acc 0.203\n",
      "batch 59: loss 3.715 | acc 0.195\n",
      "batch 60: loss 3.726 | acc 0.195\n",
      "batch 61: loss 3.823 | acc 0.164\n",
      "batch 62: loss 3.699 | acc 0.148\n",
      "batch 63: loss 3.619 | acc 0.188\n",
      "batch 64: loss 3.959 | acc 0.164\n",
      "batch 65: loss 3.598 | acc 0.188\n",
      "batch 66: loss 3.819 | acc 0.164\n",
      "batch 67: loss 3.516 | acc 0.250\n",
      "batch 68: loss 3.595 | acc 0.234\n",
      "batch 69: loss 3.708 | acc 0.227\n",
      "batch 70: loss 3.811 | acc 0.172\n",
      "batch 71: loss 3.811 | acc 0.188\n",
      "batch 72: loss 3.843 | acc 0.203\n",
      "batch 73: loss 3.586 | acc 0.281\n",
      "batch 74: loss 3.671 | acc 0.258\n",
      "batch 75: loss 3.309 | acc 0.328\n",
      "batch 76: loss 3.456 | acc 0.234\n",
      "batch 77: loss 3.700 | acc 0.227\n",
      "batch 78: loss 3.554 | acc 0.250\n",
      "batch 79: loss 3.400 | acc 0.336\n",
      "batch 80: loss 3.428 | acc 0.305\n",
      "batch 81: loss 3.561 | acc 0.273\n",
      "batch 82: loss 3.454 | acc 0.219\n",
      "batch 83: loss 3.750 | acc 0.180\n",
      "batch 84: loss 3.583 | acc 0.188\n",
      "batch 85: loss 3.454 | acc 0.281\n",
      "batch 86: loss 3.365 | acc 0.258\n",
      "batch 87: loss 3.304 | acc 0.281\n",
      "batch 88: loss 3.482 | acc 0.211\n",
      "batch 89: loss 3.418 | acc 0.227\n",
      "batch 90: loss 3.362 | acc 0.234\n",
      "batch 91: loss 3.485 | acc 0.195\n",
      "batch 92: loss 3.288 | acc 0.219\n",
      "batch 93: loss 3.282 | acc 0.258\n",
      "batch 94: loss 3.268 | acc 0.273\n",
      "batch 95: loss 3.283 | acc 0.281\n",
      "batch 96: loss 3.060 | acc 0.359\n",
      "batch 97: loss 3.241 | acc 0.305\n",
      "batch 98: loss 3.473 | acc 0.234\n",
      "batch 99: loss 3.368 | acc 0.266\n",
      "batch 100: loss 3.348 | acc 0.266\n",
      "batch 101: loss 3.337 | acc 0.258\n",
      "batch 102: loss 3.257 | acc 0.305\n",
      "batch 103: loss 3.458 | acc 0.266\n",
      "batch 104: loss 3.162 | acc 0.305\n",
      "batch 105: loss 3.315 | acc 0.234\n",
      "batch 106: loss 3.215 | acc 0.305\n",
      "batch 107: loss 3.157 | acc 0.328\n",
      "batch 108: loss 3.264 | acc 0.227\n",
      "batch 109: loss 3.308 | acc 0.258\n",
      "batch 110: loss 3.135 | acc 0.297\n",
      "batch 111: loss 3.137 | acc 0.328\n",
      "batch 112: loss 3.283 | acc 0.289\n",
      "batch 113: loss 3.265 | acc 0.312\n",
      "batch 114: loss 3.020 | acc 0.391\n",
      "batch 115: loss 3.285 | acc 0.258\n",
      "batch 116: loss 3.021 | acc 0.352\n",
      "batch 117: loss 3.403 | acc 0.266\n",
      "batch 118: loss 2.990 | acc 0.367\n",
      "batch 119: loss 3.035 | acc 0.383\n",
      "batch 120: loss 3.331 | acc 0.328\n",
      "batch 121: loss 3.173 | acc 0.328\n",
      "batch 122: loss 3.063 | acc 0.359\n",
      "batch 123: loss 3.302 | acc 0.281\n",
      "batch 124: loss 3.300 | acc 0.273\n",
      "batch 125: loss 3.218 | acc 0.305\n",
      "batch 126: loss 2.930 | acc 0.312\n",
      "batch 127: loss 3.127 | acc 0.344\n",
      "batch 128: loss 3.063 | acc 0.305\n",
      "batch 129: loss 3.026 | acc 0.344\n",
      "batch 130: loss 3.073 | acc 0.344\n",
      "batch 131: loss 3.106 | acc 0.328\n",
      "batch 132: loss 2.987 | acc 0.320\n",
      "batch 133: loss 3.026 | acc 0.383\n",
      "batch 134: loss 3.008 | acc 0.336\n",
      "batch 135: loss 2.725 | acc 0.461\n",
      "batch 136: loss 3.007 | acc 0.344\n",
      "batch 137: loss 2.914 | acc 0.344\n",
      "batch 138: loss 2.935 | acc 0.359\n",
      "batch 139: loss 3.150 | acc 0.297\n",
      "batch 140: loss 3.171 | acc 0.273\n",
      "batch 141: loss 2.984 | acc 0.328\n",
      "batch 142: loss 2.783 | acc 0.375\n",
      "batch 143: loss 2.851 | acc 0.320\n",
      "batch 144: loss 2.689 | acc 0.430\n",
      "batch 145: loss 2.877 | acc 0.398\n",
      "batch 146: loss 2.921 | acc 0.367\n",
      "batch 147: loss 2.484 | acc 0.445\n",
      "batch 148: loss 2.879 | acc 0.328\n",
      "batch 149: loss 2.641 | acc 0.406\n",
      "batch 150: loss 2.832 | acc 0.367\n",
      "batch 151: loss 2.834 | acc 0.367\n",
      "batch 152: loss 2.548 | acc 0.445\n",
      "batch 153: loss 2.900 | acc 0.406\n",
      "batch 154: loss 2.635 | acc 0.406\n",
      "batch 155: loss 2.792 | acc 0.398\n",
      "batch 156: loss 2.817 | acc 0.336\n",
      "batch 157: loss 2.570 | acc 0.367\n",
      "batch 158: loss 2.694 | acc 0.344\n",
      "batch 159: loss 2.879 | acc 0.328\n",
      "batch 160: loss 2.644 | acc 0.422\n",
      "batch 161: loss 2.854 | acc 0.406\n",
      "batch 162: loss 2.868 | acc 0.352\n",
      "batch 163: loss 2.365 | acc 0.453\n",
      "batch 164: loss 2.681 | acc 0.383\n",
      "batch 165: loss 2.664 | acc 0.375\n",
      "batch 166: loss 2.785 | acc 0.320\n",
      "batch 167: loss 2.723 | acc 0.398\n",
      "batch 168: loss 2.527 | acc 0.406\n",
      "batch 169: loss 2.943 | acc 0.312\n",
      "batch 170: loss 2.606 | acc 0.430\n",
      "batch 171: loss 2.920 | acc 0.281\n",
      "batch 172: loss 2.683 | acc 0.383\n",
      "batch 173: loss 2.478 | acc 0.398\n",
      "batch 174: loss 2.497 | acc 0.461\n",
      "batch 175: loss 2.564 | acc 0.430\n",
      "batch 176: loss 2.605 | acc 0.367\n",
      "batch 177: loss 2.597 | acc 0.375\n",
      "batch 178: loss 2.640 | acc 0.391\n",
      "batch 179: loss 2.395 | acc 0.453\n",
      "batch 180: loss 2.523 | acc 0.383\n",
      "batch 181: loss 2.346 | acc 0.508\n",
      "batch 182: loss 2.732 | acc 0.375\n",
      "batch 183: loss 2.573 | acc 0.453\n",
      "batch 184: loss 2.539 | acc 0.398\n",
      "batch 185: loss 2.227 | acc 0.516\n",
      "batch 186: loss 2.445 | acc 0.383\n",
      "batch 187: loss 2.602 | acc 0.352\n",
      "batch 188: loss 2.300 | acc 0.453\n",
      "batch 189: loss 2.543 | acc 0.406\n",
      "batch 190: loss 2.644 | acc 0.305\n",
      "batch 191: loss 2.375 | acc 0.453\n",
      "batch 192: loss 2.298 | acc 0.438\n",
      "batch 193: loss 2.471 | acc 0.398\n",
      "batch 194: loss 2.924 | acc 0.312\n",
      "batch 195: loss 2.642 | acc 0.359\n",
      "batch 196: loss 2.463 | acc 0.414\n",
      "batch 197: loss 2.190 | acc 0.461\n",
      "batch 198: loss 2.536 | acc 0.391\n",
      "batch 199: loss 2.320 | acc 0.375\n",
      "batch 200: loss 2.711 | acc 0.297\n",
      "batch 201: loss 2.495 | acc 0.430\n",
      "batch 202: loss 2.512 | acc 0.406\n",
      "batch 203: loss 2.293 | acc 0.445\n",
      "batch 204: loss 2.343 | acc 0.453\n",
      "batch 205: loss 2.389 | acc 0.445\n",
      "batch 206: loss 2.562 | acc 0.336\n",
      "batch 207: loss 2.157 | acc 0.523\n",
      "batch 208: loss 2.504 | acc 0.375\n",
      "batch 209: loss 2.297 | acc 0.406\n",
      "batch 210: loss 2.203 | acc 0.492\n",
      "batch 211: loss 2.283 | acc 0.422\n",
      "batch 212: loss 2.183 | acc 0.422\n",
      "batch 213: loss 2.390 | acc 0.469\n",
      "batch 214: loss 2.349 | acc 0.414\n",
      "batch 215: loss 2.495 | acc 0.438\n",
      "batch 216: loss 2.689 | acc 0.336\n",
      "batch 217: loss 2.478 | acc 0.406\n",
      "batch 218: loss 2.555 | acc 0.430\n",
      "batch 219: loss 2.326 | acc 0.508\n",
      "batch 220: loss 2.559 | acc 0.484\n",
      "batch 221: loss 2.520 | acc 0.375\n",
      "batch 222: loss 2.550 | acc 0.391\n",
      "batch 223: loss 2.187 | acc 0.438\n",
      "batch 224: loss 2.426 | acc 0.406\n",
      "batch 225: loss 2.193 | acc 0.445\n",
      "batch 226: loss 2.402 | acc 0.438\n",
      "batch 227: loss 2.334 | acc 0.422\n",
      "batch 228: loss 2.346 | acc 0.438\n",
      "batch 229: loss 2.375 | acc 0.406\n",
      "batch 230: loss 2.348 | acc 0.438\n",
      "batch 231: loss 2.350 | acc 0.445\n",
      "batch 232: loss 2.156 | acc 0.469\n",
      "batch 233: loss 2.509 | acc 0.398\n",
      "batch 234: loss 2.330 | acc 0.445\n",
      "batch 235: loss 2.405 | acc 0.414\n",
      "batch 236: loss 2.214 | acc 0.484\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch, (inputs, labels) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "        epoch_acc += torch.sum(preds == labels.data)\n",
    "        \n",
    "        if batch % batch_per_disp == 0:\n",
    "            batch_loss = loss.item()\n",
    "            batch_acc = torch.sum(preds == labels.data).item() / inputs.size(0)\n",
    "            print('batch %d: loss %.3f | acc %.3f' % (batch, batch_loss, batch_acc))\n",
    "    \n",
    "    epoch_loss = epoch_loss / len(dataset)\n",
    "    epoch_acc = epoch_acc / len(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}